{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset/challenge). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "*Successfully complete these all these objectives to earn a 2. There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "import pandas as pd\n",
    "\n",
    "yelp = pd.read_json('./review_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a quick look at the dataset \n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "stop_words = nlp.Defaults.stop_words.union(['food', 'place', 'restaurant', ' ', 'd', 'm', 'n', 's', 't', 've'])\n",
    "\n",
    "def tokenize(doc):\n",
    "        \"\"\"\n",
    "            tokenize will check for stop words, punctuation, and Pronouns\n",
    "            it will also lemmatize to reduce dimensionality\n",
    "        \"\"\"\n",
    "        lemmas = []\n",
    "        \n",
    "        doc = re.sub('[^a-zA-Z 0-9]', ' ', doc)\n",
    "        doc = nlp(doc)\n",
    "        \n",
    "        for token in doc:\n",
    "            conditions = (token.is_stop == False) and (token.is_punct == False) and (token.pos_ != 'PRON') and (token.text not in stop_words)\n",
    "            if conditions:\n",
    "                lemmas.append(token.lemma_)\n",
    "\n",
    "        return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['tokens'] = yelp['text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "      <td>[beware,    , fake, fake, fake,    , small, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "      <td>[come, lunch, Togo, Service, quick, Staff, fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "      <td>[Vegas, dozen, time, step, foot, Circus, Circu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "      <td>[go, night, close, street, party,    , good, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "      <td>[3, 5, 4, star, bad, price,   , 12, 99, lunch,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id                                             tokens  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  [beware,    , fake, fake, fake,    , small, bu...  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  [come, lunch, Togo, Service, quick, Staff, fri...  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  [Vegas, dozen, time, step, foot, Circus, Circu...  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  [go, night, close, street, party,    , good, a...  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  [3, 5, 4, star, bad, price,   , 12, 99, lunch,...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>1</td>\n",
       "      <td>[beware,    , fake, fake, fake,    , small, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>4</td>\n",
       "      <td>[come, lunch, Togo, Service, quick, Staff, fri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Vegas, dozen, time, step, foot, Circus, Circu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[go, night, close, street, party,    , good, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, 5, 4, star, bad, price,   , 12, 99, lunch,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>My family and I were hungry and this Subway is...</td>\n",
       "      <td>1</td>\n",
       "      <td>[family, hungry, Subway, open, 24, hour, guy, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>My wife and I came here with a a couple of fri...</td>\n",
       "      <td>3</td>\n",
       "      <td>[wife, come, couple, friend, sever, excited, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>The food was just OK and not anything to brag ...</td>\n",
       "      <td>2</td>\n",
       "      <td>[ok, brag, hot, item, tasty, horrible, look, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Today's visit is great!! Love and enjoy Town S...</td>\n",
       "      <td>4</td>\n",
       "      <td>[today, visit, great,   , love, enjoy, Town, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>This is the absolute worst place I have ever s...</td>\n",
       "      <td>1</td>\n",
       "      <td>[absolute, bad, stay, 43, year, life,   , time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars  \\\n",
       "0     BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      1   \n",
       "1     Came here for lunch Togo. Service was quick. S...      4   \n",
       "2     I've been to Vegas dozens of times and had nev...      3   \n",
       "3     We went here on a night where they closed off ...      1   \n",
       "4     3.5 to 4 stars\\n\\nNot bad for the price, $12.9...      4   \n",
       "...                                                 ...    ...   \n",
       "9995  My family and I were hungry and this Subway is...      1   \n",
       "9996  My wife and I came here with a a couple of fri...      3   \n",
       "9997  The food was just OK and not anything to brag ...      2   \n",
       "9998  Today's visit is great!! Love and enjoy Town S...      4   \n",
       "9999  This is the absolute worst place I have ever s...      1   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [beware,    , fake, fake, fake,    , small, bu...  \n",
       "1     [come, lunch, Togo, Service, quick, Staff, fri...  \n",
       "2     [Vegas, dozen, time, step, foot, Circus, Circu...  \n",
       "3     [go, night, close, street, party,    , good, a...  \n",
       "4     [3, 5, 4, star, bad, price,   , 12, 99, lunch,...  \n",
       "...                                                 ...  \n",
       "9995  [family, hungry, Subway, open, 24, hour, guy, ...  \n",
       "9996  [wife, come, couple, friend, sever, excited, p...  \n",
       "9997  [ok, brag, hot, item, tasty, horrible, look, d...  \n",
       "9998  [today, visit, great,   , love, enjoy, Town, S...  \n",
       "9999  [absolute, bad, stay, 43, year, life,   , time...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a closer look at our tokens \n",
    "yelp[['text', 'stars', 'tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, it will probably be best to use a `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import\n",
    "2. Instanitate\n",
    "3. Fit\n",
    "4. Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words,  \n",
    "                        ngram_range=(1,2),\n",
    "                        max_features = 8000,\n",
    "                        tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jepoy/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=8000,\n",
       "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True,\n",
       "                stop_words={' ', \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a',\n",
       "                            'about', 'above', 'across', 'after', 'afterwards',\n",
       "                            'again', 'against', 'all', 'almost', 'alone',\n",
       "                            'along', 'already', 'also', 'although', 'always',\n",
       "                            'am', 'among', 'amongst', 'amount', 'an', 'and',\n",
       "                            'another', ...},\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenize at 0x7faa7cae00d0>, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tfidf.fit(yelp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = tfidf.transform(yelp['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>z</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero star</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zucchini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120402</td>\n",
       "      <td>0.084988</td>\n",
       "      <td>0.063239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.060050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068706</td>\n",
       "      <td>0.096995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.169543</td>\n",
       "      <td>0.047870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              \\\n",
       "0  0.120402  0.084988  0.063239    0.0     0.0      0.0       0.0        0.0   \n",
       "1  0.000000  0.000000  0.000000    0.0     0.0      0.0       0.0        0.0   \n",
       "2  0.060050  0.000000  0.000000    0.0     0.0      0.0       0.0        0.0   \n",
       "3  0.068706  0.096995  0.000000    0.0     0.0      0.0       0.0        0.0   \n",
       "4  0.169543  0.047870  0.000000    0.0     0.0      0.0       0.0        0.0   \n",
       "\n",
       "                            ...  yummy  yummy     yummy      yup    z  zero  \\\n",
       "0         0.0          0.0  ...    0.0       0.0        0.0  0.0  0.0   0.0   \n",
       "1         0.0          0.0  ...    0.0       0.0        0.0  0.0  0.0   0.0   \n",
       "2         0.0          0.0  ...    0.0       0.0        0.0  0.0  0.0   0.0   \n",
       "3         0.0          0.0  ...    0.0       0.0        0.0  0.0  0.0   0.0   \n",
       "4         0.0          0.0  ...    0.0       0.0        0.0  0.0  0.0   0.0   \n",
       "\n",
       "   zero star  zone  zoo  zucchini  \n",
       "0        0.0   0.0  0.0       0.0  \n",
       "1        0.0   0.0  0.0       0.0  \n",
       "2        0.0   0.0  0.0       0.0  \n",
       "3        0.0   0.0  0.0       0.0  \n",
       "4        0.0   0.0  0.0       0.0  \n",
       "\n",
       "[5 rows x 8000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "\n",
    "knn.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_review = ['''\n",
    "                This place is awesome! I would bring my whole family. music, fun and great food!\n",
    "                ''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = tfidf.transform(fake_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.        , 1.        , 1.16261826, 1.16382436, 1.16640699,\n",
       "         1.17747614, 1.1775996 , 1.18177006, 1.21499637, 1.21529348]]),\n",
       " array([[6311, 6204, 9287,  426, 1714, 9297, 4068, 3884, 5634, 7594]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.kneighbors(new.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'天氣很熱吃不下東西，今天我點了一個韓國冷面湯、餐後點了甜點，冰沙系列不會太甜膩，覺得店家很用心製作，包含擺盤精緻、佐料衛生，夏日想開胃，這是一個不錯的選擇，服務人員也很敬業，以後會常常來'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #1\n",
    "yelp.iloc[6311]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'旅行でラスベガスに来ましたがネイルがはげてるのが気になり、探したお店でした。\\n質問にも丁寧に答えてくれましたし、日本人の方も日本語が話せる方も居て、とても綺麗で居心地のいいお店でした。 \\nネイルはちはるさんと言う綺麗な方が丁寧にしてくれとても気に入りました。\\n予定になかったまつ毛エクステもお願いし、日本ではまだあまりないブラウンカラーのエクステをしてもらい、とても気に入りました。\\nまた是非マッサージなどで伺いたいと思います。'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #2\n",
    "yelp.iloc[6204]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love the music here and the Kpop videos are awesome! The food here is awesome and the waiter has great personality.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #3\n",
    "yelp.iloc[426]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Love love love. The limeade was awesome. The steak tacos were awesome. The chicken fajita tacos were awesome. Awesome awesome awesome! Can't wait to hit the chandler food truck Friday again!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #4\n",
    "yelp.iloc[335]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I went here with a group of guys for my friends bachelor party.  We got bottle service and had a great time.  The service was awesome, someone was alway around willing to help. \\n\\nThe only thing that was so-so was the music, it seemed inconsistent.  The music seemed to really take off around 3-4 AM.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #5\n",
    "yelp.iloc[9630]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Whimsical, fun and gorgeous. A very nice walkthrough - and the butterflies in the greenhouse were a fun touch.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #6\n",
    "yelp.iloc[9297]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Best pizza in Ahwatukee! Family owned. Great service.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #7\n",
    "yelp.iloc[6238]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YUM!!! I found this place when after returning something at Banana Republic. I was sick of going to the the same old places during my lunch break. I ordered the full sized bowl with terakyi chicken and veggies. It was very good. The sauce wss awesome. The prices are ok because for me, this one bowl lasts me 2 meals. The atmosphere just makes me happy. I went in on a rainy day and they were just playing this fun island music. Brought me to a little tropical island on my lunch break. The service is AMAZING. Someone kept walking around and making sure everyone was doing alright. I will be back!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #8\n",
    "yelp.iloc[6380]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The best Italian in Las Vegas. I always have Pina's braciola. Home made recipe from Sicily. You can't go wrong with this restaurant. We always bring family, friends and business clients !\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #9\n",
    "yelp.iloc[4956]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This place looks AMAZING. The art on the walls is bright and just makes you feel happy. Bring your appetite!!! Then order the fried pickles!!! They're amazing! The music is relaxed and the atmosphere rocks. \\n\\nThis restaurant is different from the rest. Check em out! You really can't beat good food, a happy atmosphere, awesome decor, and did I say: DELICIOUS food?!?  \\n\\nI'll be back soon!! This place is very family friendly so bring all of the kids! There's a lot of seating so bring a big group of friends.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similar review #10\n",
    "yelp.iloc[5634]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a piepline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier. Use that pipeline to estimate a model to predict `stars`. Use the Pipeline to predict a star rating for your fake review from Part 2. \n",
    "2. Tune the entire pipeline with a GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "pipe = Pipeline([('tfidf', tfidf), ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp['stars'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "yelp['stars'].value_counts(normalize=True).sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='stars', ylabel='count'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make it look nicer\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(yelp['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "/home/jepoy/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a1dcbf2a5dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myelp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stars'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ds-unit-4/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_features' : (500, 1000) \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, parameters , cv=2, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(yelp['text'], yelp['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.predict(fake_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will reattempt this sprint challenge and make sure that it runs.\n",
    "\n",
    "## I setup the GridSearch and it took  more that 25 mins and still going.\n",
    "\n",
    "## So i set up the code that would follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Keep the `iterations` parameter at or below 5 to reduce run time\n",
    "    - The `workers` parameter should match the number of physical cores on your machine.\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "from.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the vocubalary of the yelp data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(yelp['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bag of words representation of the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "corpus = [id2word.doc2bow(doc) for doc in yelp['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your LDA model should be ready for estimation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   iterations=5,\n",
    "                   workers=6,\n",
    "                   num_topics = 10 # You can change this parameter\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I setup the GridSearch and it took  more that 25 mins and still going.\n",
    "\n",
    "So i set up the code that would follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda, corpus, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Create more visualizations of the LDA results and provide written analysis\n",
    "* Incorporate RandomizedSearchCV into docoument classification pipeline\n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, limit, start=2, step=3, passes=5):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    path : path to input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    \n",
    "    for iter_ in range(passes):\n",
    "        for num_topics in range(start, limit, step):\n",
    "            model = LdaMulticore(corpus=corpus, num_topics=num_topics, passes=3, id2word=id2word, workers=12)\n",
    "            coherencemodel = CoherenceModel(model=model, dictionary=dictionary, corpus=corpus, coherence='u_mass')\n",
    "            coherence_values.append({'pass': iter_,\n",
    "                                    'num_topics': num_topics,\n",
    "                                    'coherence_score':coherencemodel.get_coherence()})\n",
    "\n",
    "    return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                            corpus=corpus,  \n",
    "                                            start=5, \n",
    "                                            limit=50, \n",
    "                                            step=15,\n",
    "                                            passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "topic_coherence = pd.DataFrame.from_records(coherence_values)\n",
    "\n",
    "topic_coherence.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.lineplot(x='num_topics', y='coherence_score', data=topic_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Although this is not necessarily an LDA \"result,\" coherence modeling is quite useful (when your model is good) in determining how many topics to include in your LDA topic model. Here, the highest value is the lowest number, which started at 5. At this point, it would probably be worth it to run the coherence model again with an even smaller number for the start of the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentimentalize(stars):\n",
    "    if stars == 3:\n",
    "        return \"neutral\"\n",
    "    elif stars > 3:\n",
    "        return \"positive\"\n",
    "    else: \n",
    "        return \"negative\"\n",
    "    \n",
    "yelp['sentiment'] = yelp['stars'].apply(sentimentalize)\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc_topics = [lda[doc] for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(doc):\n",
    "        d_dist = {k:0 for k in range(0,5)}\n",
    "        for t in doc:\n",
    "            d_dist[t[0]] = t[1]\n",
    "        return d_dist\n",
    "    \n",
    "new_distro = [update(d) for d in doc_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.DataFrame.from_records(new_distro)\n",
    "dft.columns = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp['primary_topic'] = dft.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.countplot(x=\"primary_topic\", hue=\"sentiment\", data=yelp)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a sentiment analysis of the LSA results to show how the sentiments range between the different top 5 topics. As expected, since my model's parameters were not hyptertuned well, the distribution of sentiment aligns closely with the general distribution of ratings; that is to say, since most of the ratings were positive to begin with, and neutral ratings of 3 were the least frequent, the sentiment between the models showed the same distribution. I think yelp reviews are particularly difficult to find good topics for, since the range of descriptors are so few that the same words repeat. It would be beneficial in the future to lower the max_df parameter of the vectorizer so that it doesn't pull words that show up in the vast majority of the reviews. This would increase the salience of specific words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "ds-unit-4 (Python3)",
   "language": "python",
   "name": "ds-unit-4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
